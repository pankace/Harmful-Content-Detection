{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "from joblib import dump\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import threading\n",
        "import statistics\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import threading\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6PLm8cMwu4vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization(input):\n",
        "    assert(type(input) == list or type(input) == np.ndarray)\n",
        "\n",
        "    processing_arm = [sub.split() for sub in input]\n",
        "    extracted_tokens = []\n",
        "    for elements in processing_arm:\n",
        "        if type(elements) == list:\n",
        "            for value in elements:\n",
        "                extracted_tokens.append(value)\n",
        "\n",
        "    release_stage = np.array(extracted_tokens)\n",
        "\n",
        "    return release_stage\n",
        "\n",
        "def stop_words_purge(stored_tokens):\n",
        "    def remove_stopwords_from_text(tokens, stop_words_applied):\n",
        "        assert (type(tokens) == list or type(tokens) == np.ndarray)\n",
        "        if type(tokens) != np.ndarray:\n",
        "            tokens = np.array(tokens)\n",
        "\n",
        "        filtered_tokens = []\n",
        "        for token in tokens:\n",
        "            if token.lower() not in stop_words_applied:\n",
        "                filtered_tokens.append(token)\n",
        "        return filtered_tokens\n",
        "\n",
        "    assert(type(stored_tokens) == list or type(stored_tokens) == np.ndarray)\n",
        "    dev_mode = 0\n",
        "\n",
        "    english_stop_words = set(stopwords.words(\"English\"))\n",
        "    spanish_stop_words = set(stopwords.words(\"Spanish\"))\n",
        "    if dev_mode == 1:\n",
        "        print(\"<------------->\")\n",
        "        print(\"Purging of spanish and english stop words in progress...\")\n",
        "        print(f\"English stop words(length: {len(english_stop_words)}): {english_stop_words}\")\n",
        "        print(f\"Spanish stop words(length: {len(spanish_stop_words)}): {spanish_stop_words}\")\n",
        "\n",
        "    post_purge = []\n",
        "    # streamlined version\n",
        "    for tokens in stored_tokens:\n",
        "        tokens_lower = [token.lower() for token in tokens]\n",
        "        english_clearance = remove_stopwords_from_text(tokens_lower, english_stop_words)\n",
        "        spanish_clearance = remove_stopwords_from_text(english_clearance, spanish_stop_words)\n",
        "        post_purge.append(spanish_clearance)\n",
        "    if dev_mode == 1:\n",
        "        print(\"Purge of Spanish and english stop words completed...\")\n",
        "        print(\"<------------->\")\n",
        "\n",
        "    return post_purge\n",
        "\n",
        "def replace_additional_dollar_signs(text):\n",
        "    assert(type(text) == str)\n",
        "    parts = text.split('$', 1)\n",
        "\n",
        "    if len(parts) > 1:\n",
        "        # If there was at least one '$', replace the rest and reconstruct the string\n",
        "        parts[1] = re.sub(r\"\\$\", \"\", parts[1])\n",
        "        return '$'.join(parts)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def dataframe_generator(description_column, results_column, c1, c2, column_name, developer_mode):\n",
        "    def data_cataloging(mutation_col_1, mutation_col_2, data_logging_01, data_logging_02):\n",
        "        assert(type(mutation_col_1) == str and type(mutation_col_2) == str)\n",
        "        assert(type(data_logging_01) == list or type(data_logging_01) == np.ndarray)\n",
        "        assert(type(data_logging_01) == list or type(data_logging_02) == np.ndarray)\n",
        "\n",
        "        dataframe = pd.DataFrame({\n",
        "            mutation_col_1: data_logging_01,\n",
        "            mutation_col_2: data_logging_02\n",
        "        })\n",
        "        return dataframe\n",
        "\n",
        "    assert(type(description_column) == list or type(description_column) == np.ndarray)\n",
        "    assert(type(results_column) == list or type(results_column) == np.ndarray)\n",
        "    assert(type(c1) == str)\n",
        "    assert(type(c2) == str)\n",
        "    assert(type(column_name) == str)\n",
        "    assert(type(developer_mode) == int)\n",
        "    assert(len(description_column) == len(results_column))\n",
        "\n",
        "    dataframe_desription = description_column\n",
        "    dataframe_specifics = results_column\n",
        "\n",
        "    dataframe = data_cataloging(c1, c2, dataframe_desription, dataframe_specifics)\n",
        "    file_name = f\"{column_name.capitalize()} data analysis results.csv\"\n",
        "\n",
        "    #saving it as a csv.\n",
        "    #-------------------\n",
        "    dataframe.to_csv(file_name, index=True)\n",
        "    # -------------------\n",
        "    if developer_mode == 1:\n",
        "        print(f\"File name: {file_name}\")\n",
        "\n",
        "def enhanced_diagnostics(column_name, input_data, developer_mode):\n",
        "    assert (type(column_name) == str)\n",
        "    assert (type(input_data) == list or type(input_data) == np.ndarray)\n",
        "    assert (type(developer_mode) == int)\n",
        "    if type(input_data) != np.ndarray:\n",
        "        input_data = np.array(input_data)\n",
        "\n",
        "    zeros = 0\n",
        "    positives = 0\n",
        "    negatives = 0\n",
        "    for values in input_data:\n",
        "        if values == 0:\n",
        "            zeros += 1\n",
        "        elif values > 0:\n",
        "            positives += 1\n",
        "        elif values < 0:\n",
        "            negatives += 1\n",
        "    percentage_of_unique = (len(set(input_data)) / len(input_data))*100\n",
        "\n",
        "    updated_name = column_name + \" enhanced diagnostics\"\n",
        "    description = [\"Number of unique values\", \"Percentage of unique values\", \"Zeros\", \"Negatives\", \"Positives\", \"Total number of raw input values\"]\n",
        "    outputs = [len(set(input_data)), f\"{percentage_of_unique}%\", zeros, negatives, positives, len(input_data)]\n",
        "    dataframe_generator(description,outputs, \"Analysis metric\", \"Result\",updated_name.upper(),developer_mode)\n",
        "\n",
        "    if developer_mode == 1:\n",
        "        if len(description) == len(outputs):\n",
        "            for element_A175 in range(len(description)):\n",
        "                print(f\"{description[element_A175]}: {outputs[element_A175]}\")\n",
        "\n",
        "def numeric_analysis_arm(column_name, input_data, developer_mode):\n",
        "    assert(type(column_name) == str)\n",
        "    assert(type(input_data) == list or type(input_data) == np.ndarray)\n",
        "    assert(type(developer_mode) == int)\n",
        "    if type(input_data) != np.ndarray:\n",
        "        input_data = np.array(input_data)\n",
        "\n",
        "    analysis_description = [\"Maximum\", \"Minimum\", \"Mean\", \"Median\", \"Mode\", \"Standard deviation\", \"Range\", \"Skew\", \"Kurtosis\", \"Variance\"]\n",
        "    analysis_results = [round(np.max(input_data), 4), round(np.min(input_data), 4), round(np.mean(input_data), 4), round(np.median(input_data), 4),\n",
        "                        round(statistics.mode(input_data), 4), round(np.std(input_data), 4), round(np.max(input_data) - np.min(input_data), 4),\n",
        "                        stats.skew(input_data),round(stats.kurtosis(input_data),4), round(statistics.variance(input_data),4)]\n",
        "    if developer_mode == 1:\n",
        "        for analysis_outputs in range(len(analysis_results)):\n",
        "            print(f\"{analysis_description[analysis_outputs]}: {analysis_results[analysis_outputs]}\")\n",
        "\n",
        "    dataframe_generator(analysis_description,analysis_results,\"Analysis metric\", \"Result\",column_name.upper(), developer_mode)\n",
        "\n",
        "def remove_emojis(text):\n",
        "    assert(type(text))\n",
        "    # Regex pattern to match all emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def link_mention_purge(text):\n",
        "    assert(type(text) == str)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove Mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove everything except letters and necessary whitespace\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def inpurity_purging_protocol(input_storage):\n",
        "    assert(type(input_storage) == list or type(input_storage) == np.ndarray)\n",
        "    if type(input_storage) != np.ndarray:\n",
        "        input_storage = np.array(input_storage)\n",
        "    cleared = []\n",
        "    for element in input_storage:\n",
        "        text_no_urls = link_mention_purge(element)\n",
        "        baseline = remove_emojis(text_no_urls).replace(\"(\",\"\").replace(\")\",\"\").strip('\"')\n",
        "        updated_baseline = re.sub(r\"(\\w)([,.!?;:()-])\", r\"\\1 \\2\", baseline)\n",
        "        purged_S01 = updated_baseline.replace(\"..\", \"\")\n",
        "        purged_S02 = purged_S01.replace('\"',\"\")\n",
        "        purged_S03 = purged_S02.replace(\";)\",\"\")\n",
        "        purged_S04 = purged_S03.replace(\"*\", \"\")\n",
        "        purged_S05 = re.sub(r\"@\\w+\", \" \", purged_S04)\n",
        "        purged_S06 = purged_S05.replace(\"  \",\" \")\n",
        "        purged_S07 = purged_S06.replace(\"!!\", \"!\")\n",
        "        purged_S08 = purged_S07.replace(\"!!!\", \"!\")\n",
        "        purged_S09 = replace_additional_dollar_signs(purged_S08)\n",
        "        purged_S10 = purged_S09.replace(\",,\",\"\")\n",
        "        purged_S11 = purged_S10.replace(\"=(\",\"\")\n",
        "        purged_S12 = purged_S11.replace(\"=>\",\"\")\n",
        "        purged_S13 = purged_S12.replace(\" .\", \"\")\n",
        "        purged_S14 = purged_S13.replace(\"!\",\"\")\n",
        "        cleared.append(purged_S14)\n",
        "\n",
        "    cleared_numpy_conversion = np.array(cleared)\n",
        "\n",
        "    return cleared_numpy_conversion"
      ],
      "metadata": {
        "id": "dtEC7o9Fu382"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_from_text(tokens, stop_words_applied):\n",
        "    assert(type(tokens) == list or type(tokens) == np.ndarray)\n",
        "    if type(tokens) != np.ndarray:\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.lower() not in stop_words_applied:\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens\n",
        "\n",
        "def ml_diagnostics(y_test, predictions):\n",
        "    assert(type(y_test) == list or type(y_test) == np.ndarray)\n",
        "    assert(type(predictions) == list or type(predictions) == np.ndarray)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "\n",
        "    details = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
        "    results = [round(accuracy, 4), round(precision, 4), round(recall, 4), round(f1, 4)]\n",
        "    if len(details) == len(results):\n",
        "        print(\"Diagnostic metrics\")\n",
        "        for elements in range(len(results)):\n",
        "            print(f\"{details[elements]}: {results[elements]}\")\n",
        "\n",
        "    if (accuracy > 0.9) and (precision > 0.9):\n",
        "        print()\n",
        "        print(f\"Go touch come grass. You got accuracy to reach {round(accuracy, 4)} and precision to reach {round(precision, 4)}\")\n",
        "        if (recall > 0.8) and (f1 > 0.8):\n",
        "            print(\"Bruv. Go live life outside. This is already accurate like you had OCD writing this.\")\n",
        "            print(f\"Recall is {round(recall, 4)} and F1 score is {round(f1, 4)}. Go touch some grass. Seriously.\")\n",
        "\n",
        "def gradient_booster(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    predictions = clf.predict(X_test)\n",
        "\n",
        "    return y_test, predictions, clf\n",
        "\n",
        "def upgraded_gradient_booster(X, y):\n",
        "    if type(y) != np.ndarray:\n",
        "        y = np.array(y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "    #Baseline classifier\n",
        "    base_clf = DecisionTreeClassifier(max_depth=1)\n",
        "    #AdaBoost paired with blase classifier\n",
        "    ada_clf = AdaBoostClassifier(base_estimator=base_clf, n_estimators=50, algorithm=\"SAMME.R\", learning_rate=0.666)\n",
        "    ada_clf.fit(X_train, y_train)\n",
        "    predictions  = ada_clf.predict(X_test)\n",
        "\n",
        "    return y_test, predictions, ada_clf"
      ],
      "metadata": {
        "id": "ShUpqPMQyhZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purification and analysis"
      ],
      "metadata": {
        "id": "8uChhBtUuKKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "purification_analysis_deploy = 0"
      ],
      "metadata": {
        "id": "wQPcZfZXueQH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XljGqhjGuI-S"
      },
      "outputs": [],
      "source": [
        "if purification_analysis_deploy == 1:\n",
        "    # Uses the english translations of the tweets\n",
        "    file_path = r\"C:\\Users\\Tomy\\PycharmProjects\\Experiment - 7\\Industrial machine learning course files\\Racism classification\\Data analysis\\tranlated_tweets.csv\"\n",
        "    source_data = pd.read_csv(file_path)\n",
        "    developer_mode = 1\n",
        "    loop_viewer = 0\n",
        "\n",
        "    print(\"<------------------>\")\n",
        "    column_names = list(source_data.columns)\n",
        "    column_data_types_detected = []\n",
        "    object_data_columns = []\n",
        "    analysis_compatible_columns = []\n",
        "\n",
        "    for c_names in column_names:\n",
        "        column_data = source_data[c_names].to_numpy()\n",
        "        types_detected = column_data.dtype\n",
        "        column_data_types_detected.append(types_detected)\n",
        "        if types_detected == \"int64\":\n",
        "            analysis_compatible_columns.append(c_names)\n",
        "        elif types_detected == \"object\":\n",
        "            object_data_columns.append(c_names)\n",
        "\n",
        "    if developer_mode == 1:\n",
        "        if len(column_names) == len(column_data_types_detected):\n",
        "            for element in range(len(column_names)):\n",
        "                print(f\"Column name: {column_names[element]} | Data type detected: {column_data_types_detected[element]}\")\n",
        "    elif developer_mode == 0:\n",
        "        print(\"Developer mode inactive\")\n",
        "    print(\"<------------------>\")\n",
        "\n",
        "    text_isolation = source_data[\"Description Cleaned Translated\"].to_numpy()\n",
        "    text_isolation = inpurity_purging_protocol(text_isolation)\n",
        "    unique_locations = []\n",
        "    for tweets in text_isolation:\n",
        "        if tweets not in unique_locations:\n",
        "            unique_locations.append(tweets)\n",
        "\n",
        "    target_isolation = source_data[\"Analysis results\"].to_numpy()\n",
        "    numbers_detected = []\n",
        "    for numbers in target_isolation:\n",
        "        if numbers not in numbers_detected:\n",
        "            numbers_detected.append(numbers)\n",
        "        elif numbers in numbers_detected:\n",
        "            continue\n",
        "\n",
        "    if len(target_isolation) == len(text_isolation):\n",
        "        print(f\"Target isolated data: {len(target_isolation)}\")\n",
        "        print(f\"Text isolated data: {len(text_isolation)}\")\n",
        "\n",
        "    zero = 0\n",
        "    one = 0\n",
        "    asociation_text = []\n",
        "    asociation_target = []\n",
        "    if len(target_isolation) == len(text_isolation):\n",
        "        for elements in range(len(target_isolation)):\n",
        "            asociation_text.append(text_isolation[elements])\n",
        "            asociation_target.append(target_isolation[elements])\n",
        "            if target_isolation[elements] == 0:\n",
        "                zero += 1\n",
        "            elif target_isolation[elements] == 1:\n",
        "                one += 1\n",
        "\n",
        "    full_set = one + zero\n",
        "    if developer_mode == 1:\n",
        "        print(f\"Flagged: {one}\")\n",
        "        print(f\"Cleared: {zero}\")\n",
        "        print(f\"Full set: {full_set}\")\n",
        "\n",
        "    if full_set != 0:\n",
        "        zero_percentage = (zero / full_set) * 100\n",
        "        one_percentage = (one / full_set) * 100\n",
        "    else:\n",
        "        zero_percentage = \"Infinity\"\n",
        "        one_percentage = \"Infinity\"\n",
        "\n",
        "    if one > zero:\n",
        "        description = [\"Cleared\", \"Flagged\", \"Percentage ratio of Cleared\", \"Percentage ratio of Flagged\"]\n",
        "        results = [zero, one, f\"{round(zero_percentage, 2)} %\", f\"{round(one_percentage, 2)} %\"]\n",
        "    else:\n",
        "        description = [\"Cleared\", \"Flagged\", \"Percentage ratio of flagged\", \"Percentage ratio of Cleared\"]\n",
        "        results = [zero, one, f\"{round(one_percentage, 2)} %\", f\"{round(zero_percentage, 2)} %\"]\n",
        "\n",
        "    # Token analysis with stop words purge integrated\n",
        "    text_isolation = text_isolation.flatten()\n",
        "    tokenized_conversion = tokenization(text_isolation)\n",
        "\n",
        "    # Emoji removal in classic for-loop format\n",
        "    post_purge_storage = []\n",
        "    for pre_purge in tokenized_conversion:\n",
        "        post_purge = remove_emojis(pre_purge)\n",
        "        post_purge_storage.append(post_purge)\n",
        "\n",
        "    # Calculate unique tokens after stop words removal\n",
        "    unique_tokens, counts = np.unique(post_purge_storage, return_counts=True)\n",
        "\n",
        "    max_value = max(counts)\n",
        "    index_coordinates = 0\n",
        "    for values in range(len(counts)):\n",
        "        if counts[values] == max_value:\n",
        "            index_coordinates = values\n",
        "            break\n",
        "\n",
        "    # Out of the box random analysis\n",
        "    print(f\"Max value: {max_value}\")\n",
        "    print(f\"Index coordinate of max value: {index_coordinates}\")\n",
        "    print(f\"Token at corresponding coordinate: {post_purge_storage[index_coordinates]}\")\n",
        "\n",
        "    # Zips everything into a dictionary\n",
        "    token_counts = dict(zip(unique_tokens, counts))\n",
        "\n",
        "    # Prepare for dataframe generation\n",
        "    token_keys = list(token_counts.keys())\n",
        "    token_values = list(token_counts.values())\n",
        "    token_des_column = [\"Unique tokens\", \"Total tokens\", \"Percentage of unique tokens\"]\n",
        "    token_res_column = [len(unique_tokens), sum(counts), f\"{(len(unique_tokens) / sum(counts) * 100)} %\"]\n",
        "\n",
        "    # Enable this multithread deployment to get the csv files\n",
        "    # I disabled it because its not required anymore\n",
        "    multithreading_deployment = 0\n",
        "    if multithreading_deployment == 1:\n",
        "        if __name__ == \"__main__\":\n",
        "            t1 = threading.Thread(target=dataframe_generator(asociation_text, asociation_target, c1=\"Cleaned tweet\", c2=\"Tags\", column_name=\"Purification verification\", developer_mode=developer_mode))\n",
        "            t2 = threading.Thread(target=dataframe_generator(description, results, c1=\"Cleared/Flagged quantity\", c2=\"Analysis output\", column_name=\"Tags analysis\", developer_mode=developer_mode))\n",
        "            t3 = threading.Thread(target=dataframe_generator(token_des_column, token_res_column, c1=\"Token details\", c2=\"Token count\", column_name=\"Token analysis\", developer_mode=developer_mode))\n",
        "            t4 = threading.Thread(target=dataframe_generator(token_keys, token_values, c1=\"Individual words\", c2=\"Occurrences\", column_name=\"Frequency of token usage\", developer_mode=developer_mode))\n",
        "\n",
        "            threads = [t1, t2, t3, t4]\n",
        "            for individual_threads in threads:\n",
        "                individual_threads.start()\n",
        "            for initiated_threads in threads:\n",
        "                initiated_threads.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosting expriment"
      ],
      "metadata": {
        "id": "Uk9nGIRMyJIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Gradient_boosting_test = 0"
      ],
      "metadata": {
        "id": "ZI_jnFScyIY4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if Gradient_boosting_test == 1:\n",
        "    purified_data = pd.read_csv(r\"C:\\Users\\Tomy\\PycharmProjects\\Experiment - 7\\Industrial machine learning course files\\Racism classification\\Data analysis\\Results\\Token based\\Post_purging\\38\\Purification verification data analysis results.csv\")\n",
        "\n",
        "    cleaned_tweet = purified_data[\"Cleaned tweet\"].to_numpy()\n",
        "    tags = purified_data[\"Tags\"].to_numpy()\n",
        "\n",
        "    # Tokenization\n",
        "    token_conversion = []\n",
        "    for tweets in cleaned_tweet:\n",
        "        post_conversion = word_tokenize(tweets)\n",
        "        token_conversion.append(post_conversion)\n",
        "\n",
        "    print(\"Tokenization complete\")\n",
        "    for i in range(3):\n",
        "        print(token_conversion[i])\n",
        "\n",
        "    # Stopwords purge\n",
        "    english_stop_words = set(stopwords.words(\"English\"))\n",
        "    spanish_stop_words = set(stopwords.words(\"Spanish\"))\n",
        "    print(\"<------------->\")\n",
        "    print(\"Purging of spanish and english stop words in progress...\")\n",
        "    print(f\"English stop words(length: {len(english_stop_words)}): {english_stop_words}\")\n",
        "    print(f\"Spanish stop words(length: {len(spanish_stop_words)}): {spanish_stop_words}\")\n",
        "\n",
        "    stage_0 = []\n",
        "    # English stopwords removal\n",
        "    for stage_0_element in token_conversion:\n",
        "        english_cleaned_text = remove_stopwords_from_text(stage_0_element, english_stop_words)\n",
        "        stage_0.append(english_cleaned_text)\n",
        "\n",
        "    stop_words_stage_1 = []\n",
        "    # Spanish stop words removal\n",
        "    stage_1 = []\n",
        "    for stage_1_elements in stage_0:\n",
        "        spanish_cleaned_text = remove_stopwords_from_text(stage_1_elements, spanish_stop_words)\n",
        "        stage_1.append(spanish_cleaned_text)\n",
        "    print(\"Purge of Spanish and english stop words completed...\")\n",
        "    print(\"<------------->\")\n",
        "\n",
        "    # lammination\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lamminized_tokens = []\n",
        "    for post_processed_tokens in stage_1:\n",
        "        processed_tokens = []\n",
        "        for token in post_processed_tokens:\n",
        "            if token not in english_stop_words and token not in spanish_stop_words and token.isalpha():\n",
        "                # Lemmatize the token and append to the result list\n",
        "                lemmatized_token = lemmatizer.lemmatize(token)\n",
        "                processed_tokens.append(lemmatized_token)\n",
        "        lamminized_tokens.append(processed_tokens)\n",
        "\n",
        "\n",
        "    preprocessed_texts = [\" \".join(tokens) for tokens in lamminized_tokens]\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X = tfidf_vectorizer.fit_transform(preprocessed_texts)\n",
        "    y = tags\n",
        "    print(\"...\")\n",
        "    print(f\"X type: {type(X)}\")\n",
        "    print(f\"y type: {type(y)}\")\n",
        "    print(\"...\")\n",
        "\n",
        "    standard = 0\n",
        "    if standard == 1:\n",
        "        print(\"Baseline deployed\")\n",
        "        y_test_output, predicted_results, clf = gradient_booster(X, y)\n",
        "        ml_diagnostics(y_test_output, predicted_results)\n",
        "        dump(clf, \"Standard_baseline_boosted_model.joblib\")\n",
        "    elif standard == 0:\n",
        "        print(\"Upgraded baseline deployed\")\n",
        "        y_test_2_output, predicted_2, ada_clf = upgraded_gradient_booster(X, y)\n",
        "        ml_diagnostics(y_test_2_output, predicted_2)\n",
        "        dump(ada_clf,\"ada_boosted_model.joblib\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Benchmark(label encoder = True)\n",
        "--------------\n",
        "Diagnostic metrics\n",
        "Accuracy: 0.74\n",
        "Precision: 0.7405405405405405\n",
        "Recall: 0.7098445595854922\n",
        "F1: 0.7248677248677249\n",
        "\n",
        "Alternative alteration\n",
        "n_estimators=100, learning_rate=0.1, max_depth=5\n",
        "------------------\n",
        "Diagnostic metrics\n",
        "Accuracy: 0.76\n",
        "Precision: 0.7975460122699386\n",
        "Recall: 0.6735751295336787\n",
        "F1: 0.7303370786516852\n",
        "\n",
        "Benchmark to beat\n",
        "----------\n",
        "Upgraded baseline deployed\n",
        "Diagnostic metrics\n",
        "Accuracy: 0.9133333333333333\n",
        "Precision: 0.9466666666666667\n",
        "Recall: 0.8875\n",
        "F1: 0.9161290322580645"
      ],
      "metadata": {
        "id": "ogKX6f0GyWhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forrest test"
      ],
      "metadata": {
        "id": "kMNGrBni0Q4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import nltk\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "start_time = time.time()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix, recall_score\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJa9s6Kz0UXY",
        "outputId": "78d51978-4917-4d63-aac8-2d28b1974974"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_classifier(X, y):\n",
        "    def error_metrics(y_true, y_pred):\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        recall = recall_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "        return accuracy, precision, recall, f1\n",
        "\n",
        "    if type(y) != np.ndarray:\n",
        "        y = np.array(y)\n",
        "\n",
        "    upgrade_deployment = 0\n",
        "    if upgrade_deployment == 1:\n",
        "        svd = TruncatedSVD(n_components=78)\n",
        "        X = svd.fit_transform(X)\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    # Prediction\n",
        "    prediction = model.predict(X_test)\n",
        "    accuracy_metric, precision_metric, recall_metric, f1_metric = error_metrics(y_test, prediction)\n",
        "    return accuracy_metric, precision_metric, recall_metric, f1_metric, prediction, model\n",
        "\n",
        "def remove_stopwords_from_text(tokens, stop_words_applied):\n",
        "    assert(type(tokens) == list or type(tokens) == np.ndarray)\n",
        "    if type(tokens) != np.ndarray:\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.lower() not in stop_words_applied and token.lower() not in spanish_stop_words:\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "PERqsat50WUJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_forrest_classifier = 0"
      ],
      "metadata": {
        "id": "fdAdl0Fo0fRP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if random_forrest_classifier == 1:\n",
        "    purified_data = pd.read_csv(r\"C:\\Users\\Tomy\\PycharmProjects\\Experiment - 7\\Industrial machine learning course files\\Racism classification\\Data analysis\\Results\\Token based\\Post_purging\\32\\Purification verification data analysis results.csv\")\n",
        "\n",
        "    cleaned_tweet = purified_data[\"Cleaned tweet\"].to_numpy()\n",
        "    tags = purified_data[\"Tag\"].to_numpy()\n",
        "\n",
        "    # Tokenization\n",
        "    token_conversion = []\n",
        "    for tweets in cleaned_tweet:\n",
        "        post_conversion = word_tokenize(tweets)\n",
        "        token_conversion.append(post_conversion)\n",
        "\n",
        "    print(\"Tokenization complete\")\n",
        "    for i in range(3):\n",
        "        print(token_conversion[i])\n",
        "\n",
        "    # Stopwords purge\n",
        "    english_stop_words = set(stopwords.words(\"English\"))\n",
        "    spanish_stop_words = set(stopwords.words(\"Spanish\"))\n",
        "    print(\"<------------->\")\n",
        "    print(\"Purging of spanish and english stop words in progress...\")\n",
        "    print(f\"English stop words(length: {len(english_stop_words)}): {english_stop_words}\")\n",
        "    print(f\"Spanish stop words(length: {len(spanish_stop_words)}): {spanish_stop_words}\")\n",
        "\n",
        "    stage_0 = []\n",
        "    # English stopwords removal\n",
        "    for stage_0_element in token_conversion:\n",
        "        english_cleaned_text = remove_stopwords_from_text(stage_0_element, english_stop_words)\n",
        "        stage_0.append(english_cleaned_text)\n",
        "\n",
        "    stop_words_stage_1 = []\n",
        "    # Spanish stop words removal\n",
        "    stage_1 = []\n",
        "    for stage_1_elements in stage_0:\n",
        "        spanish_cleaned_text = remove_stopwords_from_text(stage_1_elements, spanish_stop_words)\n",
        "        stage_1.append(spanish_cleaned_text)\n",
        "    print(\"Purge of Spanish and english stop words completed...\")\n",
        "    print(\"<------------->\")\n",
        "\n",
        "\n",
        "    # lammination\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lamminized_tokens = []\n",
        "    for post_processed_tokens in stage_1:\n",
        "        processed_tokens = []\n",
        "        for token in post_processed_tokens:\n",
        "            if token not in english_stop_words and token not in spanish_stop_words and token.isalpha():\n",
        "                # Lemmatize the token and append to the result list\n",
        "                lemmatized_token = lemmatizer.lemmatize(token)\n",
        "                processed_tokens.append(lemmatized_token)\n",
        "        lamminized_tokens.append(processed_tokens)\n",
        "\n",
        "    preprocessed_texts = [\" \".join(tokens) for tokens in lamminized_tokens]\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X = tfidf_vectorizer.fit_transform(preprocessed_texts)\n",
        "    y = tags\n",
        "    if type(y) != np.ndarray:\n",
        "        y = np.array(y)\n",
        "    print(f\"X type: {type(X)}\")\n",
        "    print(f\"y type: {type(y)}\")\n",
        "    accuracy_metric, precision_metric, recall_metric, f1_metric, predictions, model = binary_classifier(X, y)\n",
        "    dump(model, \"Random_forest_model.joblib\")\n",
        "    end_time = time.time()\n",
        "    print(f\"Accuracy: {round(accuracy_metric, 4)}\")\n",
        "    print(f\"Precision: {round(precision_metric, 4)}\")\n",
        "    print(f\"Recall: {round(recall_metric, 4)}\")\n",
        "    print(f\"F1: {round(f1_metric, 4)}\")\n",
        "    processing_time = end_time - start_time\n",
        "    if processing_time < 60:\n",
        "        print(f\"Processing time: {processing_time} seconds\")\n",
        "    elif processing_time >= 60:\n",
        "        hours_conversion = (processing_time)/60\n",
        "        if hours_conversion > 1 and hours_conversion < 2:\n",
        "            print(f\"Processing time: {hours_conversion} hour\")\n",
        "        else:\n",
        "            print(f\"Processing time: {hours_conversion} hours\")\n",
        "\"\"\"\n",
        "Benchmark to beat:\n",
        "------------------\n",
        "It meets the minimum standard of 76.0% <= X <= 1.0% on every evaluation metric\n",
        "Accuracy: 0.79\n",
        "Precision: 0.7943262411347518\n",
        "Recall: 0.7671232876712328\n",
        "F1: 0.7804878048780488\n",
        "Predictions:\n",
        "[0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0\n",
        " 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0\n",
        " 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1\n",
        " 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1\n",
        " 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
        " 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1\n",
        " 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1\n",
        " 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1\n",
        " 1 0 0 1]\n",
        "Processing time: 4.091778993606567 seconds\n",
        "\n",
        "Observation: Any attempt to increase accuracy makes these metrics decrease. Any assist is welcomed.\n",
        "Note: The processing time fluctuates drastically from 3.45 seconds to sometimes 8 seconds. Do not consider that an important consideration or anything if it stays in the seconds for processing time.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RLV6DipU0Sz1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
